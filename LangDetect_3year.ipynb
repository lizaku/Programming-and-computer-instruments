{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------\n",
    "\n",
    "* **Множество случаев** — тексты на разных языках\n",
    "* **Множество классов** — языки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень простой и неплохой по качеству метод. Сначала создаем частотный словарь для всех языков и берем самые частотные слова. После этого для каждого текста, который нам надо расклассифицировать, смотрим, частотных слов какого языка в нем больше - тот язык и выбираем.\n",
    "\n",
    "Метод неплохо работает на текстах длиннее 50 слов и быстро имлементируется. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпусов и текстов для тестирования будем использовать статьи Википедии на разных языках. Скачать Википедию можно различными способами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Дампы википедии: https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "* wikiextractor: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n",
    "\n",
    "* annotated_wikiextractor: https://github.com/jodaiber/Annotated-WikiExtractor\n",
    "\n",
    "* wikipedia: https://pypi.python.org/pypi/wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачаем немного википедии для тестов\n",
    "Воспользуемся пакетом *wikipedia*:\n",
    "\n",
    "`pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Будини\n",
      "Skipping page Аттінья (значення)\n",
      "Skipping page Луквиця (річка)\n",
      "Skipping page Сома\n",
      "Skipping page Оксид вуглецю\n",
      "Skipping page Арасланово (Мелеузівський район)\n",
      "Skipping page SSI\n",
      "uk 93\n",
      "Skipping page Адамавічы\n",
      "Skipping page Кавалёнак\n",
      "Skipping page Стэфан V\n",
      "Skipping page Лойка\n",
      "Skipping page Вірус\n",
      "Skipping page Кучына\n",
      "be 94\n",
      "Skipping page Lacerta montana\n",
      "Skipping page Borovinići\n",
      "Skipping page Jalisco (homonymie)\n",
      "Skipping page Borja Fernández\n",
      "Skipping page Nostradamus (homonymie)\n",
      "Skipping page GAD\n",
      "Skipping page Raw and Uncut\n",
      "fr 93\n"
     ]
    }
   ],
   "source": [
    "import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in ('kk', 'uk', 'be', 'fr'): # казахский в википедии — это kk,\n",
    "                                      # украинский — uk, а белорусский — be\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мурза (Ница тармағы)\n",
      "Мурза (Гильдеевка) — Ресейдегі өзен. Свердлов облысы жер аумақтарынан ағып өтеді. Өзен сағасы Ница өзенінің сол жағалауынан 129 км қашықтықта орналасқан. Өзен ұзындығы 61 км-ді құрайды.\n",
      "\n",
      "\n",
      " Су реестрінің мәліметтері \n",
      "Ресей мемлекеттік су тізілімінің мәліметі бойынша Ертіс су алабы өңіріне жатады, өзеннің сушаруашылық бөлігі — Ница өзені Реж өзені және Нейва өзені өзендерінің қосылған жерінен сағасына дейін. Өзен саласы — Тобыл, өзен алабы — Ертіс.\n",
      "Ресей су ресурстары федералды агенттігі дайындаған РФ территориясын сушаруашылығы бойынша аудандастыру жөніндегі геоақпараттық жүйе мәліметтері бойынша:\n",
      "Мемлекеттік су реестріндегі су объектісінің коды — 14010501912111200007224\n",
      "Гидрологиялық тұрғыдан зерттелу (ГЗ) коды — 111200722\n",
      "Су алабының коды — 14.01.05.019\n",
      "ГЗ томының нөмірі — 11\n",
      "ГЗ бойынша шығарылуы — 2\n",
      "\n",
      "\n",
      " Дереккөздер \n",
      "\n",
      "\n",
      " Сыртқы сілтемелер \n",
      "Ресей Федерациясы Табиғи ресурстар және экология министрлігі\n",
      "Huile essentielle de citron\n",
      "L’huile essentielle de citron (Citrus limonum) est très populaire dans bon nombre de pays d'Europe. En particulier en Espagne où elle est utilisée contre les fièvres et les infections. Le citron a été employé pour traiter le scorbut à bord des navires anglais, mais aussi le paludisme et la typhoïde.\n",
      "\n",
      "\n",
      " Extraction \n",
      "Le citronnier possède des fleurs blanches parfumées et dentelées, avec des feuilles persistantes et ovales ; son fruit est jaune à maturité et peut être aussi gros qu'une orange. L'huile essentielle est extraite par pression à froid de la partie extérieure de l'écorce (zeste) d'un citron frais, ce qui en fait à proprement parler non une huile essentielle mais simplement une essence. Il faut environ 3 000 citrons pour produire un kilogramme d'essence de citron.\n",
      "\n",
      "\n",
      " Composition \n",
      "Dans la biochimie aromatique de cette essence de citron entrent les composés suivants :\n",
      "limonène ~ 66 % et pouvant aller jusqu'à 80 %  ;\n",
      "bêta-pinène ~ 12 % ;\n",
      "γ-terpinène ~ 9 % ;\n",
      "Furocoumarines : bergaptène, psoralènes-aldéhydes : 3 %.\n",
      "sabinène, myrcène, géranial de l'ordre de 1 à 2 % ;\n",
      "terpinolène, néral, linalol, moins de 1 % .\n",
      "\n",
      "\n",
      " Propriétés thérapeutiques \n",
      "L'essence de citron possède d'excellentes propriétés anti-infectieuses, antiseptiques, et antibactériennes en particulier contre les streptocoques et les bactéries sporulées ; elle est aussi utile comme antivirale grâce à sa capacité à stimuler les globules blancs (leucocytes) qui protègent des infections ; elle est un tonique digestif, dépuratif, et carminatif ; elle a la propriété d'être litholytique par sa capacité à dissoudre les calculs. Cette essence est un bon anticoagulant qui fluidifie la micro-circulation du sang. L'essence de citron est également reconnue pour ses vertus détoxifiantes, hypocholestérolémiante, tonique et pour son action facilitatrice dans la digestion[réf. souhaitée].\n",
      "Le citron est efficace dans le traitement du rhume et de la grippe car il est capable de réduire la température du corps (seulement après avis médical pour les affections graves).\n",
      "\n",
      "\n",
      " Utilisation en aromathérapie \n",
      "L'essence de citron est utilisée en diffusion et par voie interne. Le citron est notamment utile pour traiter l'hypertension, les varices, la mauvaise circulation sanguine, les problèmes digestifs dus à une insuffisance hépatique (migraine, somnolence après les repas, nausées) et les verrues. On l'utilise pour traiter les peaux grasses et rajeunir les peaux ternes. Le citron est également considéré comme un antidépresseur et un agent anti-microbien. L'essence de citron est une huile photosensibilisante : en cas d'application, l'exposition au soleil doit être évitée, en particulier dans les premières heures. Elle est dermo-agressive, en raison du limonène qu'elle contient qui temporise l'effet agressif des citrals.\n",
      "\n",
      "\n",
      " Notes et références \n",
      "\n",
      " Portail de la pharmacie\n",
      " Portail de la chimie\n",
      " Portail des odeurs, des senteurs et du parfum\n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['kk'][0]) # распечатаем пару текстов, чтобы убедиться, что все хорошо\n",
    "print(wiki_texts['fr'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем частотный список примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\t—\n",
      "179\tжәне\n",
      "97\t\n",
      "80\tбойынша\n",
      "71\tсу\n",
      "62\tдереккөздер\n",
      "60\tмен\n",
      "59\tкоды\n",
      "54\tсыртқы\n",
      "53\tсілтемелер\n",
      "52\tмәліметтер\n",
      "51\tбұл\n",
      "44\tжер\n",
      "41\tжалпы\n",
      "40\tболды.\n",
      "40\tөзен\n",
      "38\tжылы\n",
      "37\tсаны\n",
      "36\tқұрамына\n",
      "33\tаумағы\n",
      "33\tорналасқан.\n",
      "33\tадамды\n",
      "32\t1\n",
      "32\tалып\n",
      "31\t-\n",
      "30\tнемесе\n",
      "30\tжатқан\n",
      "30\tтұрғындарының\n",
      "29\tал\n",
      "28\tақша\n",
      "28\tхалық\n",
      "27\tж.\n",
      "27\tоның\n",
      "27\t–\n",
      "27\tкм²\n",
      "27\tжылғы\n",
      "27\t2\n",
      "27\tдеп\n",
      "26\tтапсырма\n",
      "26\tүшін\n",
      "24\tтағы\n",
      "24\tбазасындағы\n",
      "24\tресми\n",
      "24\tболып\n",
      "24\tбір\n",
      "24\tалабы\n",
      "23\tқарасты\n",
      "22\tол\n",
      "22\tресей\n",
      "21\t4\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "freqs = collections.defaultdict(lambda: 0)\n",
    "\n",
    "corpus = wiki_texts['kk']\n",
    "for article in corpus:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs[word] += 1\n",
    "\n",
    "for word in sorted(freqs, key=lambda w: freqs[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs[word], word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нужно сделать это для каждого языка и отфильтровать повторяющиеся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно загружать готовые частотные словари и классифицировать тексты, просто считая количество слов в каждом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2782\t de\n",
      "2352\tes \n",
      "2228\tde \n",
      "1520\tle \n",
      "1482\te d\n",
      "1271\t le\n",
      "1238\te l\n",
      "1202\t la\n",
      "1171\tla \n",
      "1056\tnt \n",
      "1013\tre \n",
      "985\tent\n",
      "921\tion\n",
      "908\ton \n",
      "904\tet \n",
      "825\ten \n",
      "801\t co\n",
      "794\ts d\n",
      "784\t et\n",
      "748\t en\n",
      "746\tque\n",
      "710\ttio\n",
      "673\te p\n",
      "668\tne \n",
      "666\tles\n",
      "646\t po\n",
      "643\test\n",
      "637\t pa\n",
      "636\tns \n",
      "624\tlle\n",
      "621\tur \n",
      "618\te s\n",
      "592\ter \n",
      "587\tdes\n",
      "587\t pr\n",
      "585\t l'\n",
      "579\tt d\n",
      "569\te c\n",
      "561\tant\n",
      "559\tue \n",
      "550\tpar\n",
      "548\til \n",
      "545\t à \n",
      "541\te e\n",
      "538\t du\n",
      "535\tst \n",
      "527\ts, \n",
      "525\tati\n",
      "495\t es\n",
      "494\tte \n"
     ]
    }
   ],
   "source": [
    "freqs = collections.defaultdict(lambda: 0)\n",
    "\n",
    "corpus = wiki_texts['fr']\n",
    "for article in corpus:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        freqs[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs, key=lambda n: freqs[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs[ngram], ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нужно сделать это для каждого языка и отфильтровать повторяющиеся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, как и в предыдущем методе, можно загружать готовые частотные словари n-грамм и классифицировать тексты, просто подсчитывая частотные n-граммы в каждом."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
